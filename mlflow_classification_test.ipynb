{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mlflow\n",
    "from seaborn import diverging_palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import (\n",
    "    ensemble,\n",
    "    model_selection,\n",
    "    preprocessing,\n",
    "    tree\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    average_precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    cohen_kappa_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    cross_val_score,\n",
    "    cross_val_predict\n",
    ")\n",
    "from yellowbrick.classifier import (\n",
    "    ConfusionMatrix,\n",
    "    ROCAUC\n",
    ")\n",
    "from yellowbrick.model_selection import (\n",
    "    LearningCurve \n",
    ")\n",
    "\n",
    "from imblearn.over_sampling import (\n",
    "    SMOTE, ADASYN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('bank_marketing_dataset.csv')\n",
    "\n",
    "\n",
    "target = [\n",
    "    1 if i == 'yes'\n",
    "    else 0\n",
    "    for i in df['subscribed']\n",
    "]\n",
    "\n",
    "df['target'] = target\n",
    "df['target'].value_counts()\n",
    "\n",
    "df.drop('subscribed', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "df.drop(['duration'],\n",
    "        axis=1,\n",
    "        inplace=True)\n",
    "\n",
    "# this attribute highly affects the output target (e.g., if duration=0 then y='no'). \n",
    "\n",
    "\n",
    "# nr.employed, euribor3m and emp.var.rate are all highly correlated. Let's only keep nr.employed\n",
    "df = df.drop(columns=['euribor3m', 'emp.var.rate'])\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "count = 0\n",
    "\n",
    "for col in df:\n",
    "    if df[col].dtype == 'object':\n",
    "        if len(list(df[col].unique())) <= 2:\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col])\n",
    "            count += 1\n",
    "            print(col)\n",
    "\n",
    "print('%d columns were label encoded.' % count)\n",
    "\n",
    "df = pd.get_dummies(df)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"target\"])\n",
    "y = df[\"target\"]\n",
    "\n",
    "X_train, X_val_test, y_train, y_val_test = model_selection.train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_val, X_test, y_val, y_test = model_selection.train_test_split(\n",
    "    X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_val = sc.transform(X_val)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Upsample \n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "mlflow.start_run()\n",
    "\n",
    "mlflow.log_param(\"test_size\", 0.3)\n",
    "mlflow.log_param(\"validation_size\", 0.35)\n",
    "mlflow.log_param(\"train_size\", 0.35)\n",
    "mlflow.log_param(\"random_state\", 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "def plot_roc_curve(fpr, tpr, roc_auc):\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "\n",
    "# PR curve\n",
    "def plot_pr_curve(precision, recall, average_precision):\n",
    "    plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')   \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Classification score\n",
    "def clf_score(clf, X_train, y_train, X_val, y_val, train=True):\n",
    "    if train:\n",
    "        print(\"Train Result:\\n\")\n",
    "        train_accuracy = accuracy_score(y_train, clf.predict(X_train))\n",
    "        print(\"accuracy score: {0:.4f}\\n\".format(train_accuracy))\n",
    "        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_train, clf.predict(X_train))))\n",
    "        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_train, clf.predict(X_train))))\n",
    "\n",
    "        res = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n",
    "        print(\"Average Accuracy: \\t {0:.4f}\".format(np.mean(res)))\n",
    "        print(\"Accuracy SD: \\t\\t {0:.4f}\".format(np.std(res)))\n",
    "\n",
    "        mlflow.log_metric(\"train_accuracy\", train_accuracy)\n",
    "        mlflow.log_metric(\"train_average_accuracy\", np.mean(res))\n",
    "        mlflow.log_metric(\"train_accuracy_sd\", np.std(res))\n",
    "        mlflow.log_param(\"cross_validation_folds\", 10)  \n",
    "        \n",
    "    elif train == False:\n",
    "        print(\"Validation Result:\\n\")\n",
    "        validation_accuracy = y_val, clf.predict(X_val)\n",
    "        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(validation_accuracy)))\n",
    "        \n",
    "        precision, recall, _ = precision_recall_curve(y_val, clf.predict(X_val))\n",
    "        average_precision = average_precision_score(y_val, clf.predict(X_val))\n",
    "        plot_pr_curve(precision, recall, average_precision)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_val, clf.predict(X_val))\n",
    "        roc_auc = roc_auc_score(y_val, clf.predict(X_val))\n",
    "        print(\"roc auc score: {}\\n\".format(roc_auc))\n",
    "        plot_roc_curve(fpr, tpr, roc_auc)\n",
    "        \n",
    "        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_val, clf.predict(X_val))))\n",
    "        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_val, clf.predict(X_val))))\n",
    "        ConfusionMatrixDisplay.from_estimator(clf, X_val, y_val)\n",
    "        print(\"End of validation Result\\n\")\n",
    "        mlflow.log_metric(\"validation_accuracy\", validation_accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric('recall', recall)\n",
    "        mlflow.log_metric(\"average_precision\", average_precision)\n",
    "        mlflow.log_metric('fpr', fpr)\n",
    "        mlflow.log_metric('tpr', tpr)\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "        \n",
    "        \n",
    "# Classification metrics\n",
    "def evaluation_metrics(y_actual, y_pred):\n",
    "            \n",
    "        precision, recall, _ = precision_recall_curve(y_actual, y_pred)\n",
    "        average_precision = average_precision_score(y_actual, y_pred)\n",
    "        plot_pr_curve(precision, recall, average_precision)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_actual, y_pred)\n",
    "        roc_auc = roc_auc_score(y_actual, y_pred)\n",
    "        print(\"roc auc score: {}\\n\".format(roc_auc))\n",
    "        plot_roc_curve(fpr, tpr, roc_auc)\n",
    "        \n",
    "        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_actual, y_pred)))\n",
    "        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_actual, y_pred)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "mlflow.log_param(\"colsample_bytree\", 0.8)\n",
    "mlflow.log_param(\"learning_rate\", 0.2)\n",
    "mlflow.log_param(\"max_depth\", 7)\n",
    "mlflow.log_param(\"n_estimators\", 100)\n",
    "mlflow.log_param(\"subsample\", 1.0)\n",
    "\n",
    "clf_xgb = xgb.XGBClassifier(colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, random_state=42, subsample=1.0)\n",
    "clf_xgb.fit(X_train_sm, y_train_sm)\n",
    "clf_score(clf_xgb, X_train_sm, y_train_sm, X_val, y_val, train=False)\n",
    "mlflow.sklearn.log_model(clf_xgb, \"xgb_model\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "y_test_pred = clf_xgb.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassificationReport\n",
    "\n",
    "classes = [\"0\", \"1\"]\n",
    "\n",
    "visualizer = ClassificationReport(\n",
    "    clf_xgb, classes=classes, support=True, is_fitted=True\n",
    ")\n",
    "\n",
    "visualizer.fit(X_train_sm, y_train_sm)        # Fit the visualizer and the model\n",
    "visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "visualizer.show()                       # Finalize and show the figure\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Reds\", annot_kws={\"size\": 16})\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import PrecisionRecallCurve\n",
    "# Create the visualizer, fit, score, and show it\n",
    "viz = PrecisionRecallCurve(clf_xgb, is_fitted=True)\n",
    "viz.fit(X_train_sm, y_train_sm)\n",
    "viz.score(X_test, y_test)\n",
    "viz.show()\n",
    "\n",
    "\n",
    "visualizer = ROCAUC(\n",
    "    clf_xgb, classes=classes, is_fitted=True\n",
    ")\n",
    "\n",
    "visualizer.fit(X_train_sm, y_train_sm)        # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "visualizer.show()                       # Finalize and show the figure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from yellowbrick.model_selection import ValidationCurve\n",
    "\n",
    "viz = ValidationCurve(\n",
    "    xgb.XGBClassifier(colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, random_state=42, subsample=1.0), \n",
    "    param_name=\"max_depth\",\n",
    "    param_range=np.arange(1, 11), \n",
    "    cv=5, \n",
    "    scoring=\"f1_weighted\",\n",
    "    np_jobs=8\n",
    ")\n",
    "\n",
    "# Fit and show the visualizer\n",
    "viz.fit(X_train, y_train)\n",
    "viz.poof()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "\n",
    "# Create the learning curve visualizer\n",
    "cv = StratifiedKFold(n_splits=12)\n",
    "sizes = np.linspace(0.3, 1.0, 10)\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "\n",
    "visualizer = LearningCurve(\n",
    "    xgb.XGBClassifier(colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, random_state=42, subsample=1.0), \n",
    "    cv=cv, \n",
    "    scoring='f1_weighted', \n",
    "    train_sizes=sizes, \n",
    "    n_jobs=8\n",
    ")\n",
    "\n",
    "visualizer.fit(X_train_sm, y_train_sm)        # Fit the data to the visualizer\n",
    "visualizer.poof()                       # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import DiscriminationThreshold\n",
    "\n",
    "visualizer = DiscriminationThreshold(clf_xgb, is_fitted=True)\n",
    "\n",
    "visualizer.fit(X_train_sm, y_train_sm)\n",
    "visualizer.poof() \n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "clf_svc = SVC(gamma='auto')\n",
    "clf_svc.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "# call decision_function on classifier to get scores (probas_pred)\n",
    "probas_pred = clf_svc.decision_function(X_test)\n",
    "# compute precision-recall pairs for different probability thresholds\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, probas_pred)\n",
    "# precision and recall vs. the decision threshold\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.ylim([0, 1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(clf_xgb)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "shap.summary_plot(shap_values, X, feature_names=X.columns)\n",
    "\n",
    "\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "val_precision = precision_score(y_val, y_val_pred)\n",
    "val_recall = recall_score(y_val, y_val_pred)\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "val_roc_auc = roc_auc_score(y_val, y_val_pred)\n",
    "\n",
    "mlflow.log_metric(\"val_accuracy\", val_accuracy)\n",
    "mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "mlflow.log_metric(\"val_precision\", val_precision)\n",
    "mlflow.log_metric(\"val_recall\", val_recall)\n",
    "mlflow.log_metric(\"val_f1\", val_f1)\n",
    "mlflow.log_metric(\"val_roc_auc\", val_roc_auc)\n",
    "\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLproduction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
